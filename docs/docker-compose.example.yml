version: "3.9"

# Exemple minimal pour Phase 2 (infra sans n8n)
# - Option A retenue: volume Docker nommé pour les modèles Ollama
# - Services non exposés à l'hôte; accessibles entre conteneurs via le réseau par défaut
# - Requiert Docker Desktop + WSL2 + NVIDIA Container Toolkit (gpus: all)

services:
  ollama:
    image: ollama/ollama:latest
    restart: unless-stopped
    # Option A (retenue): volume nommé pour les modèles
    volumes:
      - ollama-models:/root/.ollama
    environment:
      - OLLAMA_MODELS=/root/.ollama
    # Accès GPU (Compose) — nécessite NVIDIA Container Toolkit
    gpus: all
    # Ne pas exposer de ports; utilisable via http://ollama:11434 depuis d'autres services
    healthcheck:
      test: ["CMD", "ollama", "list"]
      interval: 30s
      timeout: 5s
      retries: 5

  # API Orchestrator (placeholder) — à implémenter ultérieurement
  api:
    image: ghcr.io/example/api-orchestrator:placeholder
    restart: unless-stopped
    depends_on:
      - ollama
    environment:
      - NOTEBOOK_GENERATION_AUTH=${NOTEBOOK_GENERERATION_AUTH}
      - OLLAMA_BASE_URL=http://ollama:11434
      - SUPABASE_URL=${SUPABASE_URL}
      - SUPABASE_SERVICE_ROLE_KEY=${SUPABASE_SERVICE_ROLE_KEY}
      # Ces variables sont typiquement lues côté Edge Functions; laissées à titre indicatif
      - DOCUMENT_PROCESSING_WEBHOOK_URL=${DOCUMENT_PROCESSING_WEBHOOK_URL}
      - ADDITIONAL_SOURCES_WEBHOOK_URL=${ADDITIONAL_SOURCES_WEBHOOK_URL}
      - NOTEBOOK_CHAT_URL=${NOTEBOOK_CHAT_URL}
    # Ne pas publier de ports (réseau interne uniquement)
    # command: ["sleep", "infinity"]

volumes:
  ollama-models:
    driver: local
