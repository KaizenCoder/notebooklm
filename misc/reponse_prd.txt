
**Vision & Objectifs**
- Objectif principal: quel problème prioritaire ce clone résout-il pour l’utilis
ateur final ? Même chose que notre book original.
- Parité: vise-t-on une parité fonctionnelle stricte avec le dépôt d’origine (ho
rs n8n) ? Oui.
- Positionnement: usage personnel, équipe, ou déploiement entreprise on-prem ? Usage personnel.
- Succès: quels critères de succès concrets pour la V1 (ex. traitement X docs, l
atence Y, zéro cloud) ? Fonctionnalités totales, complètes et à l'identique du Repo cloné.

**Cibles & Parcours**
- Personas: qui sont les utilisateurs cibles (tech/non-tech, chercheurs, juriste
s, etc.) ? Je suis le seul utilisateur.
- Cas d’usage prioritaires: top 3 scénarios (ex. lire un PDF long et poser des q
uestions, résumer un ensemble d’articles, générer un audio) ? Lire un PDF non. Vous posez des questions ? Résumé, ensemble d'articles.
- Environnements: OS cibles (Windows/macOS/Linux), CPU/GPU attendus ? Windows. Pour le LM, utilisation exclusive du GPU Rtx 3090.
- Langues: contenus et interface en FR uniquement ou multilingue (FR/EN) ? France.

**Périmètre & Contraintes**
- Remplacement n8n: uniquement substituer n8n par une API locale, sans modifier → oui
- Types de sources: PDF, texte, audio, sites web — confirme la liste exacte pour
 la V1. → parité totale du repo cloné
- Offline: exigence 100% hors-ligne stricte ou tolérance à quelques téléchargeme
nts initiaux de modèles ? tolérance à quelques téléchargements initiaux de modèles
- Compat: besoin d’être compatible avec les Edge Functions actuelles et leur for
mat d’I/O tel quel ? non
- Licences: contraintes de licence (models/containers) à respecter explicitement
 ? mit

**Fonctionnel (Features)**
- Chat RAG: citations obligatoires au format actuel (indices + mapping source_id
/lines), confirmé ? oui
- Indexation: chunking (taille/overlap) et embeddings via Ollama `nomic-embed-te
xt` comme l’original, confirmé ? comme original
- Génération: modèle par défaut pour LLM (ex. qwen3:8b-q4_K_M) — confirmé ou par
amétrable ? paramétrable, tous les modèles sont disponibles ici : «D:\modeles_llm\» :ou doivent téléchargés ici : « D:\modeles_llm\ »
- Audio: génération podcast via Coqui incluse en V1 ou V1.1 ? V1.1
- Transcription: Whisper ASR obligatoire pour audio en V1 ? V1.1
- Exports: besoin d’export Markdown/JSON des insights/citations dans la V1 ? V1.1

**Données, API & Intégrations**
- Schéma: garde-t-on strictement le schéma Supabase fourni (tables, RPC `match_d
ocuments`) ? postrgresql
- Historique: `n8n_chat_histories` conservé tel quel (format JSON) ? json
- Stockage: bucket `sources` (privé) et `audio` (privé) inchangés ? Règles RLS i
dentiques ? identiques
- Variables d’env: on remplace uniquement les `*_WEBHOOK_URL` n8n par l’API loca
le; autre chose à prévoir ? api local
- Observabilité: besoins min (logs par étape) vs tableau de bord ultérieur ? logs par étapes

**Non-fonctionnel & Déploiement**
- Performances: cibles approximatives (ex. indexer 50 pages < N minutes sur CPU/
GPU donné) ?
- Mémoire/VRAM: contraintes matérielles minimales (CPU-only vs GPU requis) ? gpu only, cpu interdit
- Sécurité: périmètre réseau — API limitée au réseau Docker local, auth par head
er uniquement, suffisant ? contexte mono utilisateur
- Installation: priorité à un script unique (type start_services.py) ou document
ation manuelle ?
- Tests d’acceptation: quels scénarios feront foi (jeu de PDFs tests, Q/A attend
ues, vérif citations) ? 
- Roadmap: livrables V1 (parité + rempla n8n), V1.1 (file de jobs, UX admin), V1
.2 (formats DOCX/HTML avancés) — valides ?

Si tu valides/complètes ces points, je produis une V1 de PRD structurée et explo
itable immédiatement (vision, scope, exigences, métriques, critères d’acceptatio
n, risques, milestones).
